---
---

@string{aps = {American Physical Society,}}


@inproceedings{ignatov2020aim,
  title={Aim 2020 challenge on learned image signal processing pipeline},
  author={Ignatov, Andrey and Timofte, Radu and Zhang, Zhilu and Liu, Ming and Wang, Haolin and Zuo, Wangmeng and Zhang, Jiawei and Zhang, Ruimao and Peng, Zhanglin and Koishekenov, Yeskendir and others},
  booktitle={Computer Vision--ECCV 2020 Workshops: Glasgow, UK, August 23--28, 2020, Proceedings, Part III 16},
  pages={152--170},
  year={2020},
  organization={Springer},
  arxiv={2011.04994},
  abbr={ECCV},
  abstract={This paper reviews the second AIM learned ISP challenge and provides the description of the proposed solutions and results. The participating teams were solving a real-world RAW-to-RGB mapping problem, where to goal was to map the original low-quality RAW images captured by the Huawei P20 device to the same photos obtained with the Canon 5D DSLR camera. The considered task embraced a number of complex computer vision subtasks, such as image demosaicing, denoising, white balancing, color and contrast correction, demoireing, etc. The target metric used in this challenge combined fidelity scores (PSNR and SSIM) with solutions' perceptual results measured in a user study. The proposed solutions significantly improved the baseline results, defining the state-of-the-art for practical image signal processing pipeline modeling.}
}

@inproceedings{
    koishekenov2023geom,
    title={Geometric Contrastive Learning},
    author={Yeskendir Koishekenov and Sharvaree Vadgama* and Ricardo Valperga*  and Erik J Bekkers},
    abbr={ICCV},
    booktitle={ICCV 2023 - Visual Inductive Priors for Data-Efficient Deep Learning workshop (Oral)},
    month=October,
    year={2023},
    abstract={Contrastive learning has been a long-standing research area due to its versatility and importance in learning representations. Recent works have shown improved results if the learned representations are constrained to be on a hypersphere. However, this \textit{prior geometric constraint} is not fully utilized during training. In this work, we propose making use of geodesic distances on the hypersphere to learn contrasts between representations. Through empirical results, we show that this contrastive learning approach improves downstream tasks across different contrastive learning frameworks. We show that having geometric inductive priors perform even better in contrastive learning if used along with other correct geometric information.},
    selected={true},
    website={https://openreview.net/forum?id=cE4BY5XrzR}
}



@inproceedings{koishekenov-etal-2023-memory,
    title = "Memory-efficient {NLLB}-200: Language-specific Expert Pruning of a Massively Multilingual Machine Translation Model",
    author = "Koishekenov, Yeskendir  and
      Berard, Alexandre  and
      Nikoulina, Vassilina",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = July,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.198",
    pages = "3567--3585",
    abstract = "The recently released NLLB-200 is a set of multilingual Neural Machine Translation models that cover 202 languages. The largest model is based on a Mixture of Experts architecture and achieves SoTA results across many language pairs. It contains 54.5B parameters and requires at least four 32GB GPUs just for inference.In this work, we propose a pruning method that enables the removal of up to 80{\%} of experts without further finetuning and with a negligible loss in translation quality, which makes it feasible to run the model on a single 32GB GPU. Further analysis suggests that our pruning metrics can identify language-specific experts.",
    selected={true},
    website={https://aclanthology.org/2023.acl-long.198/},
    abbr={ACL},
    arxiv={2212.09811},
    code={https://github.com/naver/nllb-pruning},
}

@inproceedings{
    koishekenov2023an,
    title={An Exploration of Conditioning Methods in Graph Neural Networks},
    author={Yeskendir Koishekenov and Erik J Bekkers},
    abbr={ICLR},
    booktitle={ICLR 2023 - Machine Learning for Drug Discovery workshop},
    month = May,
    year={2023},
    url={https://openreview.net/forum?id=11vXmgtP8iF},
    website={https://sites.google.com/view/mldd-2023/accepted-papers_1},
    abstract={The flexibility and effectiveness of message passing based graph neural networks (GNNs) induced considerable advances in deep learning on graph-structured data. In such approaches, GNNs recursively update node representations based on their neighbors and they gain expressivity through the use of node and edge attribute vectors. E.g., In computational tasks such as physics and chemistry usage of edge attributes such as relative position or distance proved to be essential. In this work, we address not what kind of attributes to use, but how to condition on this information to improve model performance. We consider three types of conditioning; weak, strong, and pure, which respectively relate to concatenation-based conditioning, gating, and transformations that are causally dependent on the attributes. This categorization provides a unifying viewpoint on different classes of GNNs, from separable convolutions to various forms of message passing networks. We provide an empirical study on the effect of conditioning methods in several tasks in computational chemistry.},
    selected={true},
    code={https://github.com/YeskendirK/conditioning-GNNs}
}

@article{koishekenov2023reducing,
  title={Reducing Over-smoothing in Graph Neural Networks Using Relational Embeddings},
  author={Koishekenov, Yeskendir},
  journal={Accepted to The Ninth International Workshop on Deep Learning on Graphs: Method and Applications (DLG-AAAI'23)},
  arxiv={2301.02924},
  abbr={AAAI},
  code={https://github.com/YeskendirK/Reducing-Oversmoothing},
  month = February,
  year={2023},
  abstract={Graph Neural Networks (GNNs) have achieved a lot of success with graph-structured data. However, it is observed that the performance of GNNs does not improve (or even worsen) as the number of layers increases. This effect has known as over-smoothing, which means that the representations of the graph nodes of different classes would become indistinguishable when stacking multiple layers. In this work, we propose a new simple, and efficient method to alleviate the effect of the over-smoothing problem in GNNs by explicitly using relations between node embeddings. Experiments on real-world datasets demonstrate that utilizing node embedding relations makes GNN models such as Graph Attention Network more robust to over-smoothing and achieves better performance with deeper GNNs. Our method can be used in combination with other methods to give the best performance. GNN applications are endless and depend on the user's objective and the type of data that they possess. Solving over-smoothing issues can potentially improve the performance of models on all these tasks.},
  selected={true},
  website={https://deep-learning-graphs.bitbucket.io/dlg-aaai23/publications.html}
}

@inproceedings{de2021reproducibility,
  title={Reproducibility study of" Data-Driven Methods for Balancing Fairness and Efficiency in Ride-Pooling"},
  author={De Boer*, Sarah and Cosma*, Radu Alexandru and Knobel*, Lukas and Koishekenov*, Yeskendir and Shaffrey*, Benjamin},
  booktitle={ML Reproducibility Challenge 2021 (Fall Edition)},
  pdf={https://zenodo.org/record/6574637/files/article.pdf},
  abbr={ReScience},
  code={https://github.com/reproducibilityaccount/reproducing-ridesharing},
  abstract={Our work attempts to verify two methods to mitigate forms of inequality in ride‐pooling platforms proposed in the paper Data-Driven Methods for Balancing Fairness and Efficiency in Ride-Pooling: (1) integrating fairness constraints into the objective functions and (2) redistributing income of drivers. We extend this paper by testing for robustness to a change in the neighbourhood selection process by using actual Manhattan neighbour‐ hoods and we use corresponding demographic data to examine differences in service based on ethnicity.},
  year={2022}
}






